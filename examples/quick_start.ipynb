{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from EasyLM import LinearModel, ModelComparator, PlotHelper\n",
    "\n",
    "# 1. GENERATE DATA\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "X1 = np.random.randn(n)\n",
    "X2 = np.random.randn(n)\n",
    "X3 = np.random.randn(n)\n",
    "y = 2 + 3*X1 + 1.5*X2 + np.random.randn(n) * 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7367f48a",
   "metadata": {},
   "source": [
    "This step creates a simple synthetic dataset that will be used to demonstrate how the library fits and compares linear regression models. Three predictor variables are randomly generated, while the target variable is constructed using a known linear relationship involving only the first two predictors. By working with controlled, reproducible data, users can clearly see how the models recover the true underlying structure and how irrelevant predictors behave when included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a5e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. FIT MODELS\n",
    "# Simple model (1 predictor)\n",
    "model1 = LinearModel()\n",
    "model1.fit(X1.reshape(-1, 1), y)\n",
    "model1.name = \"Simple\"\n",
    "\n",
    "# Multiple regression (2 predictors)\n",
    "model2 = LinearModel()\n",
    "model2.fit(np.column_stack([X1, X2]), y)\n",
    "model2.name = \"Multiple\"\n",
    "\n",
    "# Full model (3 predictors)\n",
    "model3 = LinearModel()\n",
    "model3.fit(np.column_stack([X1, X2, X3]), y)\n",
    "model3.name = \"Full\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a690ef",
   "metadata": {},
   "source": [
    "Here, three linear regression models are fitted with increasing levels of complexity. The first model uses a single predictor, the second uses two predictors, and the third includes all three. Each model is assigned a name for easier identification. This step showcases how the LinearModel class handles fitting, parameter estimation, and preparation for further comparison. Users can observe how additional predictors affect the coefficient estimates and overall model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17250ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. STATISTICAL COMPARISON\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*60)\n",
    "comparator = ModelComparator([model1, model2, model3])\n",
    "print(comparator.compare())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01f0f8c",
   "metadata": {},
   "source": [
    "This step generates a unified comparison table using the ModelComparator class. The table provides side-by-side metrics such as R², adjusted R², AIC, BIC, and error values. The goal is to illustrate how different models perform relative to one another and how complexity penalties influence model selection. This comparison allows users to identify the most appropriate model based on statistical evidence rather than guesswork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. VISUALIZE COMPARISONS (One-liners)\n",
    "\n",
    "# Coefficient comparison\n",
    "PlotHelper.coef_plot(model1, model2, model3)\n",
    "PlotHelper.coef_plot(model1, model2, model3, style='heatmap')\n",
    "\n",
    "# Metric comparison\n",
    "PlotHelper.metric_plot(model1, model2, model3)\n",
    "PlotHelper.metric_plot(model1, model2, model3, style='radar')\n",
    "\n",
    "# Complete dashboard\n",
    "PlotHelper.compare(model1, model2, model3)\n",
    "\n",
    "# Show all plots at once\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f52f8",
   "metadata": {},
   "source": [
    "Here, visualization utilities from PlotHelper are used to produce coefficient plots, metric comparisons, and an optional consolidated dashboard. These simple one-line commands display how coefficients differ across models and how various performance metrics compare visually. The purpose is to give users an intuitive understanding of the differences among the models and to demonstrate the convenience of generating publication-ready plots directly from the fitted model objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. INDIVIDUAL MODEL SUMMARY\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST MODEL SUMMARY (Multiple)\")\n",
    "print(\"=\"*60)\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df3496",
   "metadata": {},
   "source": [
    "Finally, a detailed model summary is printed for the model that best fits the generated data. The summary displays coefficient estimates, standard errors, significance tests, and global performance metrics. This step highlights the ability of the LinearModel class to provide a comprehensive statistical report similar to what is found in established statistical software. Users can use this output to interpret the fitted model and verify its suitability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
